To assist in the writing documentation in the future, this file will serve as a record of what keys in the conf file we have implemented in the code, and what each one does. 
Please update this file whenever you add support for new .conf keys in the code


***Fundamental model specification***

model - specifies the mapping between .bngl model files and .exp files. This has a unique syntax, model = model.bngl : data1.exp, data2.exp . It is possible to include both .exp and .con (constraint) files in the list.
bng_command - path to BNG2.pl, including the BNG2.pl file name. Default: based on the value of the BNGPATH env variable if set, otherwise, if the run includes any .bngl files, bng_command requires specification in the configuration file
copasi_command - path to CopasiSE, including the CopasiSE file name. Required if fitting includes any SBML (.xml) model files
output_dir - Directory where to save the output. Default: bnf_out
fit_type - Which fitting algorithm to use. Options: pso - Particle Swarm Optimization, de - Differential Evolution, ss - Scatter Search, bmc - Bayesian Markov chain Monte Carlo, sim - Simplex local search, sa - Simulated Annealing, pt - Parallel tempering. Default: de
objfunc - Which objective function to use. Options: chisq - Chi Squared, sos - Sum of squares, norm_sos - Sum of squares, normalized by the value at each point, ave_norm_sos - Sum of squares, normalized by the average value of the variable. Default: chisq
parallel_count - How many jobs to run in parallel. Default: Use all available cores.
cluster_type - Type of cluster used for running the fit.  Defaults to None (local fitting run).  Currently supports 'slurm' and will support 'torque'/'pbs'
scheduler_node - Node used for setting up distributed Client.  Defaults to None (local fitting run); takes a string identifying a machine on a network
worker_nodes - Nodes used for computation. Defaults to None; takes one or more strings separated by whitespace identifying machines on a network

Action commands below configure simulations to run that are not already specified in the model file. These commands are required to configure runs with SBML files, which do not support declaration of simulation actions in the files themselves.
time_course = key1: value1, key2: value2,... - Run a time course with the specified attributes. The following keys are available. time: the simulation time; step: the simulation time step (default 1); model: the model to run on (default is all of the models in the fitting run); suffix: The suffix of the data file to save. You should map the model to a .exp file of the same name (default: time_course)
For example you could write, 'timecourse = model: m1.xml, time: 60, step:5'
param_scan = key1: value1, key2: value2,... - Run a parameter scan with the specified attributes. The following keys are available. param: name of the parameter to scan, min: minimum value of the parameter, max: maximum value of the parameter, step: change in parameter value between consecutive simulations in the scan, time: amount of time that each simulation in the scan is run, logspace: 0 or 1. If 1, scan the parameter in log space,  model: the model to run on (default is all of the models in the fitting run); suffix: The suffix of the data file to save. You should map the model to a .exp file of the same name (default: param_scan)
***Parameter specification***

uniform_var = name__FREE___ min max - a uniformly distributed variable with bounds [min, max]
normal_var = name__FREE___ mu sigma - a normal-distributed variable in regular space with mean mu, std dev sigma. A box constraint to keep >= 0 is also assumed.
loguniform_var = name__FREE___ min max - a log-uniform distributed variable with bounds [min, max]. Bounds should be in regular space, eg [0.01, 100]
lognormal_var = name__FREE__ mu sigma - a log-normal distributed variable with mean mu, std dev sigma. mu, sigma are given in log base 10 space.

The following are to be used only with the simplex algorithm. Simplex should not use any of the other parameter specifications.
If you are using another algorithm with the flag refine, you still should not use these, you must set step size with simplex_step or simplex_log_step.
var = name__FREE__ init step - a variable that starts at init with an initial step size step. step is optional; defaults to simplex_step
logvar = name__FREE__ init step - a variable that starts at init with an initial step size step, that moves in log space. init and step should be given in log base 10 space. step is optional; defaults to simplex_log_step.


***General options***

refine: If 1, after fitting is completed, refine the best fit parameter set by a local search with the simplex algorithm. Default: 0
delete_old_files: If 1 - delete simulation folders immediately after they complete. 2 - Delete both old simulation folders and old sorted_params.txt result files Default: 0
num_to_output: The maximum number of PSets to write when writing the trajectory. Default: 1000000
output_every: Write the Trajectory to file every x iterations. Default: 20
wall_time_sim: Maximum time (in seconds) to wait for a simulation to finish.  Exceeding this (either for a simulation or network generation) results in an infinite objective function value.  Default: 3600
wall_time_gen: Same as wall_time_sim, but for the initial network generation.  Will cause the program to exit if exceeded.
verbosity: Specifies the amount of information output to the terminal. 0 - Quiet; user prompts and errors only. 1 - Normal; Warnings and concise progress updates. 2 - Verbose; Information and detailed progress updates. Default: 1
smoothing: Number of replicates to run for each parameter set (useful for stochastic runs). Default: 1
normalization = type [: d1.exp, d2.exp] - Indicates that simulation data must be normalized in order to compare with exp files. Choices are: 'init', 'peak', 'zero', 'unit'. If only the type is specified, the normalization is applied to all exp files. If one or more exp files included, it applies to only those exp files. Additionally, you may enclose an exp file in parentheses, and specify which columns of that exp file get normalized, like this (data1.exp: 1,3-5) or (data1.exp: var1,var2) Multiple lines with this key can be used. Default: No normalization

***Settings that apply to multiple Algorithms***

population_size: How many individuals to have in the population. Required.
max_iterations: Max number of iterations. Required.
initialization: 'rand' - initialize params randomly according to the distributions. 'lh' - For random_var's and loguniform_var's, initialize with a latin hypercube distribution, to more uniformly cover the search space.


***Simplex Settings***
Note: You also might define these if you chose a different algorithm but set the refine flag to use Simplex to refine the final fit. 

simplex_step - In initialization, we perturb each parameter by what step size? If you specify a step size for a specific variable via 'var' or 'logvar', it overrides this. Default: 1
simplex_log_step - Equivalent of simplex_step, for variables that move in log space. Default: simplex_step
simplex_reflection - When we reflect a point through the centroid, what is the ratio of dilation on the other side? Default: 1.0
simplex_expansion - If the reflected point was the global min, how far do we keep moving in that direction? (as a ratio to the initial distance to centroid) Default: 1.0
simplex_contraction - If the reflected point was not an improvement, we retry at what distance from the centroid? (as a ratio of the initial distance to centroid) Default: 0.5
simplex_shrink - If a whole iteration was unproductive, shrink the simplex by setting simplex point s[i] to x*s[i-1] + (1-x)*s[i]. This key specifies x. Default: 0.5
simplex_max_iterations - If specified, overrides the max_iterations setting. Useful if you are using the refine flag and want max_iterations to refer to your main algorithm. 


***Differential Evolution Settings***

mutation_rate - When generating a new individual, mutate each parameter with this probability. Default: 0.5
mutation_factor - When mutating a parameter x, change it by mutation_factor*(PS1[x] - PS2[x]) where PS1 and PS2 are random other PSets in the population.  Default: 1.0
stop_tolerance - Stop the run if max(objective) / min(objective) < 1 + this value, i.e., the entire population has converged to roughly the same objective. Default: 0.002

# These are related to the more parallel-friendly "island-based" differential evolution. If all omitted, uses regular differential evolution, which is still parallelizable, but has to wait until a whole generation finishes before moving on to the next. 
islands: Number of separate populations to evolve. Default: 1
migrate_every: After this number of generations, migrate some individuals between islands. Default: 20 (but Inf if islands = 1)
num_to_migrate: How many individuals to migrate off of each island during migration? Default: 3


***Particle Swarm settings***

cognitive: Acceleration toward a particle's own best fit
social: Acceleration toward the global best fit
particle_weight: Inertia weight of particle. Default: 1

# particle swarm params related to adaptive weight changing, which may or may not be useful
particle_weight_final: The final particle weight after the adaptive changing. Default: the value of particle_weight, effectively disabling this feature. 
adaptive_n_max: After n_max "unproductive" iterations, we have moved halfway from the initial weight to the final weight. Default: 30
adaptive_n_stop: Afer this many "unproductive" iterations, stop the simulation. Default: Inf
adaptive_abs_tol: Parameter for checking if an iteration was "unproductive" Default: 0
adaptive_rel_tol: Parameter for checking if an iteration was "unproductive" Default: 0


***Scatter Search settings***

init_size: Number of PSets to test to generate the initial population. Default: 10 * number of variables
local_min_limit: If a point is stuck for this many iterations without improvement, it is assumed to be a local min and replaced with a random parameter set. Default: 5
reserve_size: Scatter Search maintains a latin-hypercube-distributed "reserve" of parameter sets. When it needs to pick a random new parameter set, it takes one from here, so it's not similar to a previous random choice. The initial size of the reserve is this value. If the reserve becomes empty, we revert to truly random pset choices. Default: max_iterations


***Bayesian MCMC settings***

step_size: When proposing a Monte Carlo step, the step in n-dimensional parameter space has this length. Default: 0.2
burn_in: Ignore this many iterations at the start, to let the system equilibrate. Default: 10000
sample_every: Every x iterations, save the current PSet into the sampled population. Default: 100
beta: 1 over the "temperature" used for sampling. beta = 1 samples the true probability landscape. At lower beta (higher temperature), unfavorable moves are more often accepted, and parameter space is sampled more broadly. Default: 1

output_hist_every: Every x samples (i.e every x*sample_every iterations), save a historgram file for each variable, and the credible interval files, based on what has been sampled so far. Default: 100
hist_bins: Number of bins used when writing the histogram files. Default: 10
credible_intervals: Specify one or more numbers here. For each n, algorithm will save a file giving bounds for each variable such that in n% of the samples the variable lies within the bounds.  Default: 68 95

The following additional options configure Parallel Tempering, an extension of Bayesian MCMC when replica exchange occurs.
exchange_every: Every x iterations, perform replica exchange, swapping replicas that are adjacent in temperature with a statistically correct probability
beta_range=min max : The range of values of beta to use. The replicates will use population_size evenly spaced beta values within this range.
beta: Alternatively to specifying beta_range, you may specify population_size beta values here to use

***Simulated Annealing settings***

step_size: When proposing a Monte Carlo step, the step in n-dimensional parameter space has this length. (same key as MCMC above) Default: 0.2
beta: 1 over the starting "temperature" for all replicates. This value increases (i.e., temperature decreases) over the course of the run.
beta_max: Stop the algorithm if all replicates reach this beta = 1/temperature value. Default: Inf
cooling: Each time a move to a higher energy state is accepted, increase beta = 1/temperature by this value. Default: 0.01